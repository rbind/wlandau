---
layout: post
title: "Reproducible workflows in R"
---

When I seriously analyze data in R, I like to bundle my
materials in a reproducible workflow so that changes during
development are as smooth as possible and so others can easily replicate
the results. These are my thoughts so far, which I explain in
terms of the natural progression of the tools I like.

## Knitr

<a href="http://yihui.name/knitr/">Knitr</a> is the state of the art of
reproducibility in R. Code is dynamically woven into reports, and code chunks can be cached
to speed development. I use <a href="http://yihui.name/knitr/">knitr</a>
frequently for tutorials and minimal working examples. However, when I implement a serious project from start to finish with an overarching <a href="http://yihui.name/knitr/">knitr</a> report, the content of the analyses becomes too tied up with the document, and my work quickly becomes cumbersome. 
Maybe I am misusing the tool, but even with <a href="http://yihui.name/knitr/demo/externalization/">externalized code</a>,
 I experience much of the same messiness and bloating as with a spreadsheet program that jams together data entry and data analysis.
To stay clean, organized, and modular, and to ensure that document compilation issues
do not interfere with the rest of the project, I usually compartmentalize and separate reports, results, and code. That brings me to the following reproducible build systems.

## GNU Make

Programmers have been using <a href="https://www.gnu.org/software/make/">GNU Make</a>
for decades to compile low-level code, and the utility extends to programmable 
workflows in general. The main selling point 
is that the intermediate stages of a project are (re)built if and only
if their dependencies change. Here's an example that makes
a file called <code>plot.pdf</code> using R scripts.
 I write the steps to make <code>plot.pdf</code> in a file
called <code>Makefile</code> below.

<pre><code>all: plot.pdf

plot.pdf: plot.R random.csv mtcars.csv
	Rscript plot.R

mtcars.csv: mtcars.R
	Rscript mtcars.R

random.csv: random.R
	Rscript random.R

clean:
	rm -f mtcars.csv random.csv plot.pdf
</code></pre>

<p>

Above, <code>plot.pdf</code> depends on <code>plot.R</code>,
<code>random.csv</code>, and <code>mtcars.csv</code>. With those dependencies available,
<code>plot.pdf</code> is constructed by executing <code>Rscript plot.R</code> in the
<a href = "http://linuxcommand.org/">command line</a>. Similarly, <code>mtcars.csv</code>
depends on <code>mtcars.R</code> and is constructed by calling <code>Rscript mtcars.R</code>
in the <a href = "http://linuxcommand.org/">command line</a>. The case of <code>random.csv</code>
is analogous.
</p>

<p>
Below are the contents of <code>plot.R</code>,
</p>

<pre><code>x <- read.csv("mtcars.csv")[["mpg"]]
y <- read.csv("random.csv")[["y"]]
pdf("plot.pdf")
plot(x, y)
dev.off()
</code></pre>

<p>
<code>mtcars.R</code>,
</p>

<pre><code>data(mtcars)
write.csv(mtcars, "mtcars.csv")
</code></pre>

and <code>random.R</code>.

<pre><code>d = data.frame(y = rnorm(32))
write.csv(d, "random.csv")
</code></pre>

<p>
With all these files, I open a command line program such as <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-the-linux-terminal">Terminal</a>,
 type <code>make</code>, and press enter. The files are generated in sequence,
 and the console shows the following.
</p>

<pre><code>Rscript random.R
Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>


<p>
If I wanted to distribute this workflow over two parallel processes, I could have typed
 <code>make -j 2</code>. In that case, <code>mtcars.csv</code> and
<code>random.csv</code> would have been computed simultaneously, and then <code>plot.pdf</code>
would have been created afterwards.
</p>

<p>
If I run <code>make</code> again, I get

<pre><code>make: Nothing to be done for 'all'.
</code></pre>

<p>
"Targets" such as <code>plot.pdf</code>, <code>mtcars.csv</code>, and
<code>random.csv</code> are (re)built if and only if their dependencies change. 
For example, suppose
I change <code>mtcars.R</code> and rerun <code>make</code>. Then, <code>mtcars.csv</code> and <code>plot.pdf</code>
are rebuilt, but <code>random.csv</code> is left alone.
</p>

<pre><code>Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>
</p>

<p>
<a href="https://www.gnu.org/software/make/">Make</a> saves a lot of time, but
the solution is incomplete. To get the most out of
<a href="https://www.gnu.org/software/make/">Make</a>'s conditional rebuild policy,
you'll have to split up your R code into as many separate files as possible and
collate them yourself, which could get messy. Also, adding comments and whitespace
triggers unnecessary rebuilds. A better solution for R is
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
<a href="https://github.com/richfitz/remake">remake</a>.
</p>


<h2>remake</h2>

<p>
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
 <a href="https://github.com/richfitz/remake">remake</a> package is like
 <a href="https://www.gnu.org/software/make/">GNU Make</a> for a single R session, and 
it removes much of the awkwardness of combining <a href="https://www.gnu.org/software/make/">Make</a> and R.
To run the previous example, I first create the <a href="http://yaml.org/">YAML</a>
file <code>remake.yml</code> below, which is analogous to a
<a href="https://www.gnu.org/software/make/">Makefile</a>.

<pre><code>sources: code.R

targets:

  all:
    depends: plot.pdf

  plot.pdf:
    command: my_plot(mtcars, random)
    plot: TRUE

  mtcars:
    command: my_mtcars()

  random:
    command: my_random()
</code></pre>
</p>

<p>
I also maintain a source file called <code>code.R</code>.
</p>

<pre><code>my_mtcars = function(){
  data(mtcars)
  mtcars
}

my_random = function(){
  data.frame(y = rnorm(32))
}

my_plot = function(mtcars, random){
  plot(mtcars$mpg, random$y)
}
</code></pre>

<p>
Then, to build the project, I open an R session and run the following.
</p>

<pre><code>library(remake)
make()
</code></pre>

<p>
Only outdated or missing targets are (re)built in future calls to <code>make()</code>. 
For example, changing
the contents of <code>my_random()</code>
does not trigger a rebuild of <code>mtcars</code>. In addition,
you can add comments and whitespace throughout <code>code.R</code>
without triggering unnecessary rebuilds. I also like that I don't have
to micromanage intermediate files. Rather than having to save CSV files,
I can let <a href="https://github.com/richfitz/remake">remake</a>
compute objects <code>mtcars</code> and <code>random</code> and maintain
them in a hidden folder called <code>.remake/objects</code>, managed internally by
<a href="https://github.com/richfitz/storr">storr</a>.
</p>

<p>
<a href="https://github.com/richfitz/remake">Remake</a> is amazing, but it
still has room for improvement. For one, it currently does not support parallel
computing, so there is not a built-in way to distribute targets over parallel processes
as with <code>make -j</code>. In addition, the user needs to maintain a
<a href="http://yaml.org/">YAML</a> file that could get large and complicated for
serious projects. With these issues in mind, I wrote the
<a href="https://github.com/wlandau/parallelRemake">parallelRemake</a> and
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
packages.
</p>

<h2>parallelRemake</h2>

<p>
The <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
package is basically <a href="https://github.com/richfitz/remake">remake</a>
plus parallel computing. You begin with a <a href="http://yaml.org/">YAML</a>
file such as <code>remake.yml</code> as in the above example, and then you
write a <a href="https://www.gnu.org/software/make/">Makefile</a>
that can run <a href="https://github.com/richfitz/remake">remake</a>
targets in parallel. Using the previous
example, I first open an R session and call <code>write_makefile()</code>.
</p>

<pre><code>library(parallelRemake)
write_makefile()
</code></pre>

<p>
That produces the <a href="https://www.gnu.org/software/make/">Makefile</a>
shown below.
</p>

<pre><code># Generated by parallelRemake::write_makefile() on 2016-06-14 08:45:47

.PHONY: all plot.pdf mtcars random clean

all: plot.pdf

plot.pdf: mtcars random
	Rscript -e 'remake::make("plot.pdf", remake_file = "remake.yml")'

mtcars:
	Rscript -e 'remake::make("mtcars", remake_file = "remake.yml")'

random:
	Rscript -e 'remake::make("random", remake_file = "remake.yml")'

clean:
	Rscript -e 'remake::make("clean", remake_file = "remake.yml")'
	rm -rf .remake
</code></pre>

<p>
I can now run the whole project in two parallel processes with
<code>make -j 2</code>, and the targets <code>mtcars</code>
and <code>random</code> are built simultaneously.
</p>

<p>
You can set up and run this example from start to finish with
<pre><code>library(parallelRemake)
run_example_parallelRemake()
</code></pre>
</p>

<p>
As before, intermediate R objects are stored in <code>.remake/objects</code>,
a cache managed internally by <a href="https://github.com/richfitz/storr">storr</a>.
To load these objects into an R session for debugging and planning purposes,
use the <code>recall()</code> function. To see the objects available,
use <code>recallable()</code>.
</p>

<h2>workflowHelper</h2>

<p>
This package is for anyone who uses R to analyze
multiple datasets in multiple ways. For a common set of use cases,
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
 makes it much easier to set up and maintain
a reproducible workflow in the midst of heavy development. The user supplies functions to
generate each dataset and perform each analysis, and then the package
puts all the pieces together and manages much of the data for you. Let's dive into an
example, which you can run from start to finish with

<pre><code>library(workflowHelper)
run_example_workflowHelper()
</code></pre>

The online file <a href="https://github.com/wlandau/workflowHelper/blob/master/inst/example/code.R"><code>code.R</code></a>
has all the building blocks (i.e. user-defined functions) of the workflow.
In practice, you can divide <code>code.R</code> into separate R files
however you want. The online file
<a href="https://github.com/wlandau/workflowHelper/blob/master/inst/example/workflow.R"><code>workflow.R</code></a>,
also below, structures the workflow.
</p>

<pre><code>library(workflowHelper)

sources = strings(code.R)
packages = strings(MASS)

# Uncomment the line below before building pdf/html.
# packages = strings(MASS, rmarkdown, tools)

datasets = commands(
  normal16 = normal_dataset(n = 16),
  poisson32 = poisson_dataset(n = 32),
  poisson64 = poisson_dataset(n = 64)
)

# For 4 replicates of each kind of dataset, 
# assign datasets = reps(datasets, 4)

analyses = commands(
  linear = linear_analysis(..dataset..),
  quadratic = quadratic_analysis(..dataset..)
)

summaries = commands(
  mse = mse_summary(..dataset.., ..analysis..),
  coef = coefficients_summary(..analysis..)
)

output = commands(
  coef_table = do.call(I("rbind"), coef),
  coef.csv = write.csv(coef_table, target_name),
  mse_vector = unlist(mse)
)

plots = commands(
  mse.pdf = hist(mse_vector, col = I("black"))
)

reports = commands(
  markdown.md = list(fig.height = 6, fig.align = "right"),
  latex.tex = TRUE
#  markdown.html = render("markdown.md", quiet = TRUE, clean = FALSE),
#  latex.pdf = texi2pdf("latex.tex", clean = FALSE)
)

begin = c("# This is my Makefile", "# Variables...")
plan_workflow(sources, packages, datasets, analyses, 
  summaries, output, plots, reports = reports, begin = begin)
</code></pre>

<p>
Running <code>workflow.R</code> creates a <a href="https://www.gnu.org/software/make/">Makefile</a> and a 
<a href="https://github.com/richfitz/remake"><code>remake</code></a>/
<a href="http://yaml.org/">YAML</a> file to

<ul>
<li>Generate some datasets.</li>
<li>Analyze each dataset with multiple methods of analysis.</li>
<li>Summarize each analysis of each dataset.</li>
<li>Generate tables and plots from the summaries.</li>
<li>Compile <a href="http://yihui.name/knitr/"><code>knitr</code></a> reports.</li>
</ul>

Then, to run the workflow with no parallel computing, open an R session and type
</p>

<pre><code>library(remake)
make(remake_file = "remake.yml")
</code></pre>

<p>
To distrubite the workflow over multiple parallel
processes, use the <a href="https://www.gnu.org/software/make/">Makefile</a> and run the project with
<code>make -j [n]</code>, where <code>[n]</code> is the number of processes. As before, changes to the functions in <code>code.R</code> 
trigger rebuilds only for the targets that need updating, and the <code>recall()</code> and
 <code>recallable()</code> functions from 
 <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
 can be used to access the <a href="https://github.com/richfitz/storr">storr</a> cache.
</p>


<h2>downsize</h2>

<p>
When I'm setting up a reproducible workflow in R, I don't run the
full scaled-up workflow right away. To save time, I first run a fast scaled-down
version to test and debug. I wrote the <a href="https://github.com/wlandau/downsize">downsize</a>
 package to help with this downsizing. The function <code>scale_down()</code>
sets the global option <code>downsize</code> to <code>TRUE</code>, and <code>scale_up()</code> sets this option to <code>FALSE</code> (default). The <code>downsize</code> option is a signal to the <code>ds()</code> function. Calling <code>ds(A, ...)</code> says "downsize <code>A</code> to a smaller object if <code>getOption("downsize")</code> is <code>TRUE</code>". Here are some examples. Try running the following lines by themselves first. Then,
enter <code>scale_down()</code> and run them again to see what changes.

<pre><code>library(downsize)
ds("Leave me alone when the downsize option is FALSE.", 
  "Return me when the downsize option is TRUE.")
ds(1:10, length = 2)
m = matrix(1:36, ncol = 6)
ds(m, ncol = 2)
ds(m, ncol = 2, random = T)
ds(m, nrow = 2)
ds(m, dim = c(2, 2))
ds(data.frame(x = 1:10, y = 1:10), nrow = 5)
dim(ds(array(0, dim = c(10, 100, 2, 300, 12)), dim = rep(3, 5)))
</code></pre>

For atomic objects and data frames, setting the <code>random</code> argument of <code>ds()</code> to <code>TRUE</code>
takes a random subset of elements instead of simply the first few.
</p>

<p>
To use <a href="https://github.com/wlandau/downsize">downsize</a>
with the <a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
example, just include <code>downsize</code> in
<code>packages</code> inside <code>workflow.R</code> and
replace the top few lines of <code>code.R</code> with the following.

<pre><code>library(downsize)
scale_down()

normal_dataset = function(n = 16){
  ds(data.frame(x = rnorm(n, 1), y = rnorm(n, 5)), nrow = 4)
}

poisson_dataset = function(n = 16){
  ds(data.frame(x = rpois(n, 1), y = rpois(n, 5)), nrow = 4)
}
</code></pre>

For the full scaled-up workflow,
just delete the first two lines or replace <code>scale_down()</code> with <code>scale_up()</code>.
Unfortunately, <a href="https://github.com/richfitz/remake">remake</a>
 does not rebuild outdated targets when global options are changed,
 so you'll have to clean up your work (<code>make clean</code>) and manually 
 start over whenever you
 toggle the <code>downsize</code> option (or switch between <code>scale_up()</code>
 and <code>scale_down()</code>).
</p>

<h2>Reproducible workflows as R packages</h2>

<p>
Until recently, I implemented reproducible workflows as 
formal R packages for the sake of portability, quality control, standardized
documentation, and <a href="http://r-pkgs.had.co.nz/tests.html">automated testing</a>. 
Unfortunately, as I show <a href="https://github.com/wlandau/remakeInPackage">here</a>,
<a href="https://github.com/richfitz/remake">remake</a> does not currently play
nicely with custom packages. Still, for now, I will happily stick with <a href="https://github.com/richfitz/remake">remake</a> and my companion toolkit.
</p>
