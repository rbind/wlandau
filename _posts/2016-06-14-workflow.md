---
layout: post
title: "Reproducible workflows in R"
---

The issue of reproducibility has been getting a lot of attention in the R community,
and I have learned a lot from the ongoing discussions.
So these days, when I seriously analyze data in R, I bundle my
materials in a reproducible workflow to smooth changes during development
and to help others replicate the results.
These are my thoughts so far, which I explain in
terms of a natural progression of tools.

## knitr

<a href="http://yihui.name">Yihui Xie</a>'s <a href="http://yihui.name/knitr/">knitr</a> package is the state of the art of
reproducibility in R. Code is dynamically woven into reports, and code chunks can be <a href="http://yihui.name/knitr/demo/cache/">cached</a>
to speed development. I have relied on <a href="http://yihui.name/knitr/">knitr</a>
frequently for tutorials, minimal working examples, graduate school homework assignments, presentations about code, and other pedagogical tasks that require as much code as possible to be on display. However, for serious projects with thousands of lines of code and computations that take several hours or days, I believe
<a href="http://yihui.name/knitr/">knitr</a> is the wrong tool for managing the entire workflow. For me, even with <a href="http://yihui.name/knitr/demo/cache/">caching</a> and <a href="http://yihui.name/knitr/demo/externalization/">externalized code</a>, a medium-to-large project quickly becomes cumbersome when implemented as a <a href="http://yihui.name/knitr/">knitr</a> report from start to finish.
To stay clean, organized, and modular, to open up more high-performance parallel computing options, and to ensure that document compilation issues do not interfere with the rest of the project, I turn to other reproducible build systems.

## GNU Make

Programmers have been using <a href="https://www.gnu.org/software/make/">GNU Make</a>
for decades to compile low-level code, and the utility extends to programmable
workflows in general. The main selling point
is that the intermediate stages of a project are (re)built if and only
if they do not exist or their dependencies change. Here's an example that makes
a file called <code>plot.pdf</code> using R scripts.
 I write the steps to make <code>plot.pdf</code> in a file
called <code>Makefile</code> below.

<pre><code>all: plot.pdf

plot.pdf: plot.R random.csv mtcars.csv
	Rscript plot.R

mtcars.csv: mtcars.R
	Rscript mtcars.R

random.csv: random.R
	Rscript random.R

clean:
	rm -f mtcars.csv random.csv plot.pdf
</code></pre>

<p>

Above, <code>plot.pdf</code> depends on <code>plot.R</code>,
<code>random.csv</code>, and <code>mtcars.csv</code>. With those dependencies available,
<code>plot.pdf</code> is constructed by executing <code>Rscript plot.R</code> in the
<a href = "http://linuxcommand.org/">command line</a>. Similarly, <code>mtcars.csv</code>
depends on <code>mtcars.R</code> and is constructed by calling <code>Rscript mtcars.R</code>
in the <a href = "http://linuxcommand.org/">command line</a>. The case of <code>random.csv</code>
is analogous.
</p>

<p>
Below are the contents of <code>plot.R</code>,
</p>

<pre><code>x <- read.csv("mtcars.csv")[["mpg"]]
y <- read.csv("random.csv")[["y"]]
pdf("plot.pdf")
plot(x, y)
dev.off()
</code></pre>

<p>
<code>mtcars.R</code>,
</p>

<pre><code>data(mtcars)
write.csv(mtcars, "mtcars.csv")
</code></pre>

and <code>random.R</code>.

<pre><code>d = data.frame(y = rnorm(32))
write.csv(d, "random.csv")
</code></pre>

<p>
With all these files, I open a command line program such as <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-the-linux-terminal">Terminal</a>,
 type <code>make</code>, and press enter. The files are generated in sequence,
 and the console shows the following.
</p>

<pre><code>Rscript random.R
Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>


<p>
If I wanted to distribute this workflow over two parallel processes, I could have typed
 <code>make -j 2</code>. In that case, <code>mtcars.csv</code> and
<code>random.csv</code> would have been computed simultaneously, and then <code>plot.pdf</code>
would have been created afterwards.
</p>

<p>
If I run <code>make</code> again, I get

<pre><code>make: Nothing to be done for 'all'.
</code></pre>

<p>
"Targets" such as <code>plot.pdf</code>, <code>mtcars.csv</code>, and
<code>random.csv</code> are (re)built if and only if they do not exist or their dependencies change.
For example, suppose
I change <code>mtcars.R</code> and rerun <code>make</code>. Then, <code>mtcars.csv</code> and <code>plot.pdf</code>
are rebuilt, but <code>random.csv</code> is left alone.
</p>

<pre><code>Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>
</p>

<p>
<a href="https://www.gnu.org/software/make/">Make</a> saves a lot of time, but
the solution is incomplete. To get the most out of
<a href="https://www.gnu.org/software/make/">Make</a>'s conditional rebuild policy,
you'll have to split up your R code into as many separate files as possible and
collate them yourself, which could get messy. Also, adding comments and whitespace
triggers unnecessary rebuilds. A better solution for R is
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
<a href="https://github.com/richfitz/remake">remake</a>.
</p>


<h2>remake</h2>

<p>
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
 <a href="https://github.com/richfitz/remake">remake</a> package is like
 <a href="https://www.gnu.org/software/make/">GNU Make</a> for a single R session, and
it removes much of the awkwardness of combining <a href="https://www.gnu.org/software/make/">Make</a> and R.
To run the previous example, I first create the <a href="http://yaml.org/">YAML</a>
file <code>remake.yml</code> below, which is analogous to a
<a href="https://www.gnu.org/software/make/">Makefile</a>.

<pre><code>sources: code.R

targets:

  all:
    depends: plot.pdf

  plot.pdf:
    command: my_plot(mtcars, random)
    plot: TRUE

  mtcars:
    command: my_mtcars()

  random:
    command: my_random()
</code></pre>
</p>

<p>
I also maintain a source file called <code>code.R</code>.
</p>

<pre><code>my_mtcars = function(){
  data(mtcars)
  mtcars
}

my_random = function(){
  data.frame(y = rnorm(32))
}

my_plot = function(mtcars, random){
  plot(mtcars$mpg, random$y)
}
</code></pre>

<p>
Then, to build the project, I open an R session and run the following.
</p>

<pre><code>library(remake)
make()
</code></pre>

<p>
Only outdated or missing targets are (re)built in future calls to <code>make()</code>.
For example, changing
the contents of <code>my_random()</code>
does not trigger a rebuild of <code>mtcars</code>. In addition,
you can add comments and whitespace throughout <code>code.R</code>
without triggering unnecessary rebuilds. I also like that I don't have
to micromanage intermediate files. Rather than saving CSV files,
I just let <a href="https://github.com/richfitz/remake">remake</a>
compute objects <code>mtcars</code> and <code>random</code> and automatically maintain
them in a hidden folder called <code>.remake/objects</code>, managed internally by
<a href="https://github.com/richfitz/storr">storr</a>.
</p>

<p>
<a href="https://github.com/richfitz/remake">Remake</a> has some idiosyncrasies, and
commands must be specified carefully. For example,
use the <code>I()</code> for string literals, and with the exception of <code>I()</code>,
avoid writing nested functions as commands. For more information, please refer to
<a href="https://github.com/wlandau/remakeGenerator/blob/master/TROUBLESHOOTING.md">TROUBLESHOOTING.md</a>
for <a href="https://github.com/wlandau/remakeGenerator">remakeGenerator</a>.
</p>

<p>
<a href="https://github.com/richfitz/remake">Remake</a> is amazing, but it
still has room for improvement. For one, it currently does not support parallel
computing, so there is not a built-in way to distribute targets over parallel processes
as with <code>make -j</code>. In addition, the user needs to maintain a
<a href="http://yaml.org/">YAML</a> file that could get large and complicated for
serious projects. With these issues in mind, I wrote the
<a href="https://github.com/wlandau/parallelRemake">parallelRemake</a> and
<a href="https://github.com/wlandau/remakeGenerator">remakeGenerator</a>
packages.
</p>

<h2>parallelRemake</h2>

<p>
The <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
package is basically <a href="https://github.com/richfitz/remake">remake</a>
plus parallel computing. You begin with a <a href="http://yaml.org/">YAML</a>
file such as <code>remake.yml</code> as in the above example, and then you
write a <a href="https://www.gnu.org/software/make/">Makefile</a>
that can run <a href="https://github.com/richfitz/remake">remake</a>
targets in parallel. Using the previous
example, I first open an R session and call <code>write_makefile()</code>.
</p>

<pre><code>library(parallelRemake)
write_makefile()
</code></pre>

<p>
That produces the <a href="https://www.gnu.org/software/make/">Makefile</a>
shown below.
</p>

<pre><code># Generated by parallelRemake::write_makefile() on 2016-06-14 08:45:47

.PHONY: all plot.pdf mtcars random clean

all: plot.pdf

plot.pdf: mtcars random
	Rscript -e 'remake::make("plot.pdf", remake_file = "remake.yml")'

mtcars:
	Rscript -e 'remake::make("mtcars", remake_file = "remake.yml")'

random:
	Rscript -e 'remake::make("random", remake_file = "remake.yml")'

clean:
	Rscript -e 'remake::make("clean", remake_file = "remake.yml")'
	rm -rf .remake
</code></pre>

<p>
I can now run the whole project in two parallel processes with
<code>make -j 2</code>, and the targets <code>mtcars</code>
and <code>random</code> are built simultaneously.
</p>

<p>
You can set up and run this example from start to finish with
<pre><code>library(parallelRemake)
run_example_parallelRemake()
</code></pre>
</p>

<p id="slurm">
If you want to run <code>make -j</code> to distribute tasks over multiple nodes of a <a href="http://slurm.schedmd.com/">Slurm cluster</a>, refer to the <a href="https://www.gnu.org/software/make/">Makefile</a> in <a href="http://plindenbaum.blogspot.com/2014/09/parallelizing-gnu-make-4-in-slurm.html">this post</a> and write

<pre><code>write_makefile(...,
  begin = c(
    "SHELL=srun",
    ".SHELLFLAGS= [ARGS] bash -c"))
</code></pre>

in an R session, where <code>[ARGS]</code> stands for additional arguments to <code>srun</code>. Then, once the <a href="https://www.gnu.org/software/make/">Makefile</a> is generated, you can run the workflow with <code>nohup make -j [N] &</code> in the command line, where <code>[N]</code> is the number of simultaneous tasks. For other task managers such as <a href="https://en.wikipedia.org/wiki/Portable_Batch_System">PBS</a>, such an approach may not be possible. Regardless of the system, be sure that all nodes point to the same working directory so that they share the same <code>.remake</code> <a href="https://github.com/richfitz/storr">storr</a> cache.
</p>

<p>
As before, intermediate R objects are stored in <code>.remake/objects</code>,
a cache managed internally by <a href="https://github.com/richfitz/storr">storr</a>.
To load these objects into an R session for debugging and planning purposes,
use the <code>recall()</code> function. To see the objects available,
use <code>recallable()</code>.
</p>

<h2>remakeGenerator</h2>

<p>
The <a href="https://github.com/wlandau/remakeGenerator">remakeGenerator</a> package is for generating <a href="https://github.com/richfitz/remake"><code>remake</code></a>-style workflows with minimal code. It is especially useful for analyzing
multiple datasets multiple ways reproducibly in computationally-demanding workflows under heavy development. Let's dive into an
example, which you can run from start to finish with

<pre><code>library(remakeGenerator)
example_remakeGenerator(1)
</code></pre>

The online file <a href="https://github.com/wlandau/remakeGenerator/blob/master/inst/example1/code.R"><code>code.R</code></a>
has all the building blocks (i.e. user-defined functions), and
<a href="https://github.com/wlandau/remakeGenerator/blob/master/inst/example1/workflow.R"><code>workflow.R</code></a>, also below, is the master plan.
</p>

<pre><code>library(remakeGenerator)

datasets = commands(
  normal16 = normal_dataset(n = 16),
  poisson32 = poisson_dataset(n = 32),
  poisson64 = poisson_dataset(n = 64))

analyses = analyses(
  commands = commands(
    linear = linear_analysis(..dataset..),
    quadratic = quadratic_analysis(..dataset..)),
  datasets = datasets)

summaries = summaries(
  commands = commands(
    mse = mse_summary(..dataset.., ..analysis..),
    coef = coefficients_summary(..analysis..)),
  analyses = analyses, datasets = datasets, gather = strings(c, rbind))

output = commands(coef.csv = write.csv(coef, target_name))

plots = commands(mse.pdf = hist(mse, col = I("black")))
plots$plot = TRUE

reports = data.frame(target = strings(markdown.md, latex.tex),
  depends = c("poisson32, coef, coef.csv", ""))
reports$knitr = TRUE

targets = targets(datasets = datasets, analyses = analyses, summaries = summaries,
  output = output, plots = plots, reports = reports)

workflow(targets, sources = "code.R", packages = "MASS",
  begin = c("# Prepend this", "# to the Makefile."))

###############################################
### Now, run remake::make() or the Makefile ###
###############################################
</code></pre>

<p>
To distribute the workflow over multiple parallel
processes, use the <a href="https://www.gnu.org/software/make/">Makefile</a> and run the project with
<code>make -j [n]</code>, where <code>[n]</code> is the number of processes. To deploy parallel processes across multiple nodes of a <a href="http://slurm.schedmd.com/">Slurm</a> cluster, refer to the <a href="#slurm">parallelRemake instructions</a>.
</p>

<p>
As before, changes to the functions in <code>code.R</code>
trigger rebuilds only for the targets that need updating, and the <code>recall()</code> and
 <code>recallable()</code> functions from
 <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
 can be used to access the <a href="https://github.com/richfitz/storr">storr</a> cache.
</p>

<p>There is a <a href="https://github.com/wlandau/remakeGenerator/tree/master/inst/example2">second example</a> that opens up more extensibility for workflows that don't fit the mold. See the <a href="https://github.com/wlandau/remakeGenerator/blob/master/README.md">README</a> for more information.</p>

<h2>downsize</h2>

<p>
With the <a href="https://cran.r-project.org/web/packages/downsize/"><code>downsize</code></a> package,
you can toggle the test and production versions of your workflow with the
flip of a <code>TRUE/FALSE</code> global option. This is helpful when your workflow takes a long time to run, you want to test it quickly, and <a href="https://www.r-bloggers.com/unit-testing-with-r/">unit testing</a> is too reductionist to cover everything. Just intersperse your code with calls to the <a href="https://cran.r-project.org/web/packages/downsize/"><code>downsize()</code></a> function. For example, say you want to analyze a large dataset.

<pre><code>big_data <- data.frame(x = rnorm(1e4), y = rnorm(1e4))
</code></pre>

But for the sake of time, you want to test and debug your code on a smaller dataset. In your code, select your dataset with a call to `downsize()`.

<pre><code>library(downsize)
my_data <- downsize(big_data)
</code></pre>

The <code>downsize()</code> function provides a choice. Above, <code>my_data</code> becomes
<code>big_data</code> if <code>getOption("downsize")</code> is <code>FALSE</code> or <code>NULL</code> (default) and <code>head(big_data)</code> if <code>getOption("downsize")</code> is <code>TRUE</code>. You can toggle the global option <code>downsize</code> with a call to <code>scale_up()</code> or <code>scale_down()</code>, and you can override the option with <code>downsize(..., downsize = L)</code>, where <code>L</code> is <code>TRUE</code> or <code>FALSE</code>. Check if you are in test mode or production mode with the <code>scaling()</code> function.
</p>

<p>
For instance, take the following script.

<pre><code>library(downsize)
scale_down() # scales the workflow appropriately
scaling() # shows if the workflow is scaled up or down
big_data <- data.frame(x = rnorm(1e4), y = rnorm(1e4)) # always large
my_data <- downsize(big_data) # either large or small
nrow(my_data) # responds to scale_down() and scale_up()
# ...more code, time-consuming if my_data is large...
</code></pre>

The workflow is in test mode because <code>scale_down()</code> is called. To scale up to production mode,
replace <code>scale_down()</code> with <code>scale_up()</code> and <b>keep everything else exactly the same</b>.
You can verify the change by seeing that <code>nrow(my_data)</code> returns 10000 instead of 6. In this way,
tedium and human error are avoided when scaling up, and the test provides a close approximation to the actual task at hand.


</p>

<p>You also have the option to provide a replacement for <code>big_data</code>.

<pre><code>small_data <- data.frame(x = runif(16), y = runif(16))
my_data <- downsize(big_data, small_data)
</code></pre>

<p>Similarly, you can conditionally subset your data or toggle entire code blocks
based on the scaling of the workload. See the
<a href="https://cran.r-project.org/web/packages/downsize//blob/master/vignettes/downsize.Rmd">package vignette</a>
for examples.
</p>

<p>
The <a href="https://cran.r-project.org/web/packages/downsize/">downsize</a> package is compatible with <a href="https://github.com/wlandau/remakeGenerator">remakeGenerator</a> workflows, and the <a href="https://github.com/wlandau/remakeGenerator/blob/master/vignettes/remakeGenerator.Rmd">remakeGenerator vignette</a> suggests one of many potential ways to use the two together.
</p>

<h2>packrat</h2>

In mid-August of 2016, Eric Nantz of the <a href="https://r-podcast.org/">R-Podcast</a> converted me to <code><a href="https://rstudio.github.io/packrat/">packrat</a></code> (by <a href="https://kevinushey.github.io/">Kevin Ushey</a> and others at <a href="https://www.rstudio.com/">RStudio</a>), a package that lengthens the shelf life of R projects. <code><a href="https://rstudio.github.io/packrat/">Packrat</a></code> maintains local snapshots of dependencies so that your project won't break when external packages are updated. <code><a href="https://rstudio.github.io/packrat/">Packrat</a></code> is fully compatible with all the tools I explained in this post. Just be sure your current working directory is the root directory of your project when you run <code>remake::make()</code> or the <a href="https://www.gnu.org/software/make/">Makefile</a>. You can learn more about <code><a href="https://rstudio.github.io/packrat/">packrat</a></code> with the <a href="https://rstudio.github.io/packrat/walkthrough.html">hands-on walkthrough</a>.

<h2>The R package system </h2>

<p>
Until recently, I implemented reproducible workflows as
formal R packages for the sake of portability, quality control, standardized
documentation, and <a href="http://r-pkgs.had.co.nz/tests.html">automated testing</a>.
Unfortunately, as I show <a href="https://github.com/wlandau/remakeInPackage">here</a>,
<a href="https://github.com/richfitz/remake">remake</a> does not currently play
nicely with custom packages. Still, for now, I happily choose <a href="https://github.com/richfitz/remake">remake</a> and my companion toolkit.
</p>

<h2>ProjectTemplate</h2>

<p>
The <a href="http://projecttemplate.net/"><code>ProjectTemplate</code></a> package
is structurally similar to the R package system, but with an explicit focus on user-defined workflows. In addition,
whereas <a href="https://github.com/richfitz/remake">remake</a> focuses on the
reproducibility of declarative workflows,
<a href="http://projecttemplate.net/"><code>ProjectTemplate</code></a> improves
the file management and ergonomics of workflows that may either be interactive or in
batch form. In the future <a href="http://projecttemplate.net/"><code>ProjectTemplate</code></a> and <a href="https://github.com/richfitz/remake">remake</a> might work together, and you can follow updates <a href="https://github.com/johnmyleswhite/ProjectTemplate/issues/154">here</a> and
<a href="https://github.com/richfitz/remake/issues/98">here</a>.
</p>
