---
layout: post
title: "Reproducible workflows in R"
---

The issue of reproducibility has been getting a lot of attention in the R community,
and I have learned a lot from the ongoing discussions.
So these days, when I seriously analyze data in R, I bundle my
materials in a reproducible workflow to smooth changes during development
and to help others replicate the results. 
These are my thoughts so far, which I explain in
terms of a natural progression of tools.

## Knitr

<a href="http://yihui.name/knitr/">Knitr</a> is the state of the art of
reproducibility in R. Code is dynamically woven into reports, and code chunks can be <a href="http://yihui.name/knitr/demo/cache/">cached</a>
to speed development. I have relied on <a href="http://yihui.name/knitr/">knitr</a>
frequently for tutorials, minimal working examples, graduate school homework assignments, presentations about code, and other pedagogical tasks that require as much code as possible to be on display. However, for serious projects with thousands of lines of code and computations that take several hours or days, I believe 
<a href="http://yihui.name/knitr/">knitr</a> is the wrong tool for managing the entire workflow. For me, even with <a href="http://yihui.name/knitr/demo/cache/">caching</a> and <a href="http://yihui.name/knitr/demo/externalization/">externalized code</a>, a medium-to-large project quickly becomes cumbersome when implemented as a <a href="http://yihui.name/knitr/">knitr</a> report from start to finish.
To stay clean, organized, and modular, to open up more high-performance parallel computing options, and to ensure that document compilation issues do not interfere with the rest of the project, I turn to other reproducible build systems.

## GNU Make

Programmers have been using <a href="https://www.gnu.org/software/make/">GNU Make</a>
for decades to compile low-level code, and the utility extends to programmable 
workflows in general. The main selling point 
is that the intermediate stages of a project are (re)built if and only
if they do not exist or their dependencies change. Here's an example that makes
a file called <code>plot.pdf</code> using R scripts.
 I write the steps to make <code>plot.pdf</code> in a file
called <code>Makefile</code> below.

<pre><code>all: plot.pdf

plot.pdf: plot.R random.csv mtcars.csv
	Rscript plot.R

mtcars.csv: mtcars.R
	Rscript mtcars.R

random.csv: random.R
	Rscript random.R

clean:
	rm -f mtcars.csv random.csv plot.pdf
</code></pre>

<p>

Above, <code>plot.pdf</code> depends on <code>plot.R</code>,
<code>random.csv</code>, and <code>mtcars.csv</code>. With those dependencies available,
<code>plot.pdf</code> is constructed by executing <code>Rscript plot.R</code> in the
<a href = "http://linuxcommand.org/">command line</a>. Similarly, <code>mtcars.csv</code>
depends on <code>mtcars.R</code> and is constructed by calling <code>Rscript mtcars.R</code>
in the <a href = "http://linuxcommand.org/">command line</a>. The case of <code>random.csv</code>
is analogous.
</p>

<p>
Below are the contents of <code>plot.R</code>,
</p>

<pre><code>x <- read.csv("mtcars.csv")[["mpg"]]
y <- read.csv("random.csv")[["y"]]
pdf("plot.pdf")
plot(x, y)
dev.off()
</code></pre>

<p>
<code>mtcars.R</code>,
</p>

<pre><code>data(mtcars)
write.csv(mtcars, "mtcars.csv")
</code></pre>

and <code>random.R</code>.

<pre><code>d = data.frame(y = rnorm(32))
write.csv(d, "random.csv")
</code></pre>

<p>
With all these files, I open a command line program such as <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-the-linux-terminal">Terminal</a>,
 type <code>make</code>, and press enter. The files are generated in sequence,
 and the console shows the following.
</p>

<pre><code>Rscript random.R
Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>


<p>
If I wanted to distribute this workflow over two parallel processes, I could have typed
 <code>make -j 2</code>. In that case, <code>mtcars.csv</code> and
<code>random.csv</code> would have been computed simultaneously, and then <code>plot.pdf</code>
would have been created afterwards.
</p>

<p>
If I run <code>make</code> again, I get

<pre><code>make: Nothing to be done for 'all'.
</code></pre>

<p>
"Targets" such as <code>plot.pdf</code>, <code>mtcars.csv</code>, and
<code>random.csv</code> are (re)built if and only if they do not exist or their dependencies change. 
For example, suppose
I change <code>mtcars.R</code> and rerun <code>make</code>. Then, <code>mtcars.csv</code> and <code>plot.pdf</code>
are rebuilt, but <code>random.csv</code> is left alone.
</p>

<pre><code>Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>
</p>

<p>
<a href="https://www.gnu.org/software/make/">Make</a> saves a lot of time, but
the solution is incomplete. To get the most out of
<a href="https://www.gnu.org/software/make/">Make</a>'s conditional rebuild policy,
you'll have to split up your R code into as many separate files as possible and
collate them yourself, which could get messy. Also, adding comments and whitespace
triggers unnecessary rebuilds. A better solution for R is
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
<a href="https://github.com/richfitz/remake">remake</a>.
</p>


<h2>remake</h2>

<p>
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
 <a href="https://github.com/richfitz/remake">remake</a> package is like
 <a href="https://www.gnu.org/software/make/">GNU Make</a> for a single R session, and 
it removes much of the awkwardness of combining <a href="https://www.gnu.org/software/make/">Make</a> and R.
To run the previous example, I first create the <a href="http://yaml.org/">YAML</a>
file <code>remake.yml</code> below, which is analogous to a
<a href="https://www.gnu.org/software/make/">Makefile</a>.

<pre><code>sources: code.R

targets:

  all:
    depends: plot.pdf

  plot.pdf:
    command: my_plot(mtcars, random)
    plot: TRUE

  mtcars:
    command: my_mtcars()

  random:
    command: my_random()
</code></pre>
</p>

<p>
I also maintain a source file called <code>code.R</code>.
</p>

<pre><code>my_mtcars = function(){
  data(mtcars)
  mtcars
}

my_random = function(){
  data.frame(y = rnorm(32))
}

my_plot = function(mtcars, random){
  plot(mtcars$mpg, random$y)
}
</code></pre>

<p>
Then, to build the project, I open an R session and run the following.
</p>

<pre><code>library(remake)
make()
</code></pre>

<p>
Only outdated or missing targets are (re)built in future calls to <code>make()</code>. 
For example, changing
the contents of <code>my_random()</code>
does not trigger a rebuild of <code>mtcars</code>. In addition,
you can add comments and whitespace throughout <code>code.R</code>
without triggering unnecessary rebuilds. I also like that I don't have
to micromanage intermediate files. Rather than saving CSV files,
I just let <a href="https://github.com/richfitz/remake">remake</a>
compute objects <code>mtcars</code> and <code>random</code> and automatically maintain
them in a hidden folder called <code>.remake/objects</code>, managed internally by
<a href="https://github.com/richfitz/storr">storr</a>.
</p>

<p>
<a href="https://github.com/richfitz/remake">Remake</a> is amazing, but it
still has room for improvement. For one, it currently does not support parallel
computing, so there is not a built-in way to distribute targets over parallel processes
as with <code>make -j</code>. In addition, the user needs to maintain a
<a href="http://yaml.org/">YAML</a> file that could get large and complicated for
serious projects. With these issues in mind, I wrote the
<a href="https://github.com/wlandau/parallelRemake">parallelRemake</a> and
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
packages.
</p>

<h2>parallelRemake</h2>

<p>
The <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
package is basically <a href="https://github.com/richfitz/remake">remake</a>
plus parallel computing. You begin with a <a href="http://yaml.org/">YAML</a>
file such as <code>remake.yml</code> as in the above example, and then you
write a <a href="https://www.gnu.org/software/make/">Makefile</a>
that can run <a href="https://github.com/richfitz/remake">remake</a>
targets in parallel. Using the previous
example, I first open an R session and call <code>write_makefile()</code>.
</p>

<pre><code>library(parallelRemake)
write_makefile()
</code></pre>

<p>
That produces the <a href="https://www.gnu.org/software/make/">Makefile</a>
shown below.
</p>

<pre><code># Generated by parallelRemake::write_makefile() on 2016-06-14 08:45:47

.PHONY: all plot.pdf mtcars random clean

all: plot.pdf

plot.pdf: mtcars random
	Rscript -e 'remake::make("plot.pdf", remake_file = "remake.yml")'

mtcars:
	Rscript -e 'remake::make("mtcars", remake_file = "remake.yml")'

random:
	Rscript -e 'remake::make("random", remake_file = "remake.yml")'

clean:
	Rscript -e 'remake::make("clean", remake_file = "remake.yml")'
	rm -rf .remake
</code></pre>

<p>
I can now run the whole project in two parallel processes with
<code>make -j 2</code>, and the targets <code>mtcars</code>
and <code>random</code> are built simultaneously.
</p>

<p>
You can set up and run this example from start to finish with
<pre><code>library(parallelRemake)
run_example_parallelRemake()
</code></pre>
</p>

<p id="slurm">
If you want to run <code>make -j</code> to distribute tasks over multiple nodes of a <a href="http://slurm.schedmd.com/">Slurm cluster</a>, refer to the <a href="https://www.gnu.org/software/make/">Makefile</a> in <a href="http://plindenbaum.blogspot.com/2014/09/parallelizing-gnu-make-4-in-slurm.html">this post</a> and write

<pre><code>write_makefile(..., 
  begin = c(
    "SHELL=srun",
    ".SHELLFLAGS= [ARGS] bash -c"))
</code></pre>

in an R session, where <code>[ARGS]</code> stands for additional arguments to <code>srun</code>. Then, once the <a href="https://www.gnu.org/software/make/">Makefile</a> is generated, you can run the workflow with <code>nohup make -j [N] &</code> in the command line, where <code>[N]</code> is the number of simultaneous tasks. For other task managers such as <a href="https://en.wikipedia.org/wiki/Portable_Batch_System">PBS</a>, such an approach may not be possible. Regardless of the system, be sure that all nodes point to the same working directory so that they share the same <code>.remake</code> <a href="https://github.com/richfitz/storr">storr</a> cache.
</p>

<p>
As before, intermediate R objects are stored in <code>.remake/objects</code>,
a cache managed internally by <a href="https://github.com/richfitz/storr">storr</a>.
To load these objects into an R session for debugging and planning purposes,
use the <code>recall()</code> function. To see the objects available,
use <code>recallable()</code>.
</p>

<h2>workflowHelper</h2>

<p>
This package is for anyone who uses R to analyze
multiple datasets in multiple ways. For a common set of use cases,
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
 makes it much easier to set up and maintain
a reproducible workflow in the midst of heavy development. The user supplies functions to
generate each dataset and perform each analysis, and then the package
puts all the pieces together and manages much of the data for you. Let's dive into an
example, which you can run from start to finish with

<pre><code>library(workflowHelper)
run_example_workflowHelper()
</code></pre>

The online file <a href="https://github.com/wlandau/workflowHelper/blob/master/inst/example/code.R"><code>code.R</code></a>
has all the building blocks (i.e. user-defined functions) of the workflow.
In practice, you can divide <code>code.R</code> into separate R files
however you want. The online file
<a href="https://github.com/wlandau/workflowHelper/blob/master/inst/example/workflow.R"><code>workflow.R</code></a>,
also below, structures the workflow.
</p>

<pre><code>library(workflowHelper)

sources = strings(code.R)
packages = strings(MASS)

# Uncomment the line below before building pdf/html.
# packages = strings(MASS, rmarkdown, tools)

datasets = commands(
  normal16 = normal_dataset(n = 16),
  poisson32 = poisson_dataset(n = 32),
  poisson64 = poisson_dataset(n = 64)
)

# For 4 replicates of each kind of dataset, 
# assign datasets = reps(datasets, 4)

analyses = commands(
  linear = linear_analysis(..dataset..),
  quadratic = quadratic_analysis(..dataset..)
)

summaries = commands(
  mse = mse_summary(..dataset.., ..analysis..),
  coef = coefficients_summary(..analysis..)
)

output = commands(
  coef_table = do.call(I("rbind"), coef),
  coef.csv = write.csv(coef_table, target_name),
  mse_vector = unlist(mse)
)

plots = commands(
  mse.pdf = hist(mse_vector, col = I("black"))
)

reports = commands(
  markdown.md = list("poisson32", "coef_table", "coef.csv"), # dependencies
  latex.tex = TRUE # no dependencies here
#  markdown.html = render("markdown.md", quiet = TRUE, clean = FALSE),
#  latex.pdf = texi2pdf("latex.tex", clean = FALSE)
)

begin = c("# This is my Makefile", "# Variables...")
plan_workflow(sources, packages, datasets, analyses, 
  summaries, output, plots, reports = reports, begin = begin)
</code></pre>

<p>
Running <code>workflow.R</code> creates a <a href="https://www.gnu.org/software/make/">Makefile</a> and a 
<a href="https://github.com/richfitz/remake"><code>remake</code></a>/
<a href="http://yaml.org/">YAML</a> file to

<ul>
<li>Generate some datasets.</li>
<li>Analyze each dataset with multiple methods of analysis.</li>
<li>Summarize each analysis of each dataset.</li>
<li>Generate tables and plots from the summaries.</li>
<li>Compile (<a href="http://yihui.name/knitr/">knitr</a>) reports. Here, R objects <code>poisson32</code> and <code>coef_table</code> are automatically exported for use inside the code chunks of <code>markdown.Rmd</code>, and the file <code>coef.csv</code> is an additional dependency.</li>
</ul>

Then, to run the workflow with no parallel computing, open an R session and type
</p>

<pre><code>library(remake)
make(remake_file = "remake.yml")
</code></pre>

<p>
To distribute the workflow over multiple parallel
processes, use the <a href="https://www.gnu.org/software/make/">Makefile</a> and run the project with
<code>make -j [n]</code>, where <code>[n]</code> is the number of processes. To deploy parallel processes across multiple nodes of a <a href="http://slurm.schedmd.com/">Slurm</a> cluster, refer to the <a href="#slurm">parallelRemake instructions</a>.
</p>

<p>
As before, changes to the functions in <code>code.R</code> 
trigger rebuilds only for the targets that need updating, and the <code>recall()</code> and
 <code>recallable()</code> functions from 
 <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
 can be used to access the <a href="https://github.com/richfitz/storr">storr</a> cache.
</p>


<h2>downsize</h2>

<p>
When I'm setting up a reproducible workflow in R, I don't run the
full scaled-up workflow right away. To save time, I first run a fast scaled-down
version to test and debug. I wrote the <a href="https://github.com/wlandau/downsize">downsize</a>
 package to help with this downsizing. The function <code>scale_down()</code>
sets the global option <code>downsize</code> to <code>TRUE</code>, and <code>scale_up()</code> sets this option to <code>FALSE</code> (default). The <code>downsize</code> option is a signal to the <code>ds()</code> function. Calling <code>ds(A, ...)</code> says "downsize <code>A</code> to a smaller object if <code>getOption("downsize")</code> is <code>TRUE</code>". Here are some examples. Try running the following lines by themselves first. Then,
enter <code>scale_down()</code> and run them again to see what changes.

<pre><code>library(downsize)
ds("Leave me alone when the downsize option is FALSE.", 
  "Return me when the downsize option is TRUE.")
ds(1:10, length = 2)
m = matrix(1:36, ncol = 6)
ds(m, ncol = 2)
ds(m, ncol = 2, random = T)
ds(m, nrow = 2)
ds(m, dim = c(2, 2))
ds(data.frame(x = 1:10, y = 1:10), nrow = 5)
dim(ds(array(0, dim = c(10, 100, 2, 300, 12)), dim = rep(3, 5)))
</code></pre>

For atomic objects and data frames, setting the <code>random</code> argument of <code>ds()</code> to <code>TRUE</code>
takes a random subset of elements instead of simply the first few.
</p>

<p>
To use <a href="https://github.com/wlandau/downsize">downsize</a>
with the <a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
example, just include <code>downsize</code> in
<code>packages</code> inside <code>workflow.R</code> and
replace the top few lines of <code>code.R</code> with the following.

<pre><code>library(downsize)
scale_down()

normal_dataset = function(n = 16){
  ds(data.frame(x = rnorm(n, 1), y = rnorm(n, 5)), nrow = 4)
}

poisson_dataset = function(n = 16){
  ds(data.frame(x = rpois(n, 1), y = rpois(n, 5)), nrow = 4)
}
</code></pre>

For the full scaled-up workflow,
just delete the first two lines or replace <code>scale_down()</code> with <code>scale_up()</code>.
Unfortunately, <a href="https://github.com/richfitz/remake">remake</a>
 does not rebuild outdated targets when global options are changed,
 so you'll have to clean up your work (<code>make clean</code>) and manually 
 start over whenever you
 toggle the <code>downsize</code> option (or switch between <code>scale_up()</code>
 and <code>scale_down()</code>).
</p>

<h2>Reproducible workflows as R packages</h2>

<p>
Until recently, I implemented reproducible workflows as 
formal R packages for the sake of portability, quality control, standardized
documentation, and <a href="http://r-pkgs.had.co.nz/tests.html">automated testing</a>. 
Unfortunately, as I show <a href="https://github.com/wlandau/remakeInPackage">here</a>,
<a href="https://github.com/richfitz/remake">remake</a> does not currently play
nicely with custom packages. Still, for now, I happily choose <a href="https://github.com/richfitz/remake">remake</a> and my companion toolkit.
</p>


<h2>August 2016 update: packrat</h2>

In mid-August of 2016, Eric Nantz of the <a href="https://r-podcast.org/">R-Podcast</a> converted me to <code><a href="https://rstudio.github.io/packrat/">packrat</a></code> (<a href="https://kevinushey.github.io/">Kevin Ushey</a> and others at <a href="https://www.rstudio.com/">RStudio</a>), a package that lengthens the shelf life of R projects. <code><a href="https://rstudio.github.io/packrat/">Packrat</a></code> maintains local snapshots of dependencies so that your project won't break when external packages are updated. <code><a href="https://rstudio.github.io/packrat/">Packrat</a></code> is fully compatible with all the tools I explained in this post. Just be sure your current working directory is the root directory of your project when you run <code>remake::make()</code> or the <a href="https://www.gnu.org/software/make/">Makefile</a>. You can learn more about <code><a href="https://rstudio.github.io/packrat/">packrat</a></code> with the <a href="https://rstudio.github.io/packrat/walkthrough.html">hands-on walkthrough</a>.
